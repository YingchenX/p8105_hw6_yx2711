---
title: "p8105_hw6_yx2711"
author: "Yingchen Xu"
date: "2022-11-30"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
library(modelr)

set.seed(1)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	message = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```


# Problem 1

To obtain a distribution for $\hat{r}^2$, we'll follow basically the same procedure we used for regression coefficients: draw bootstrap samples; the a model to each; extract the value I'm concerned with; and summarize. Here, we'll use `modelr::bootstrap` to draw the samples and `broom::glance` to produce `r.squared` values. 

```{r weather_df, cache = TRUE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```


```{r, cache=TRUE}
weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

In this example, the $\hat{r}^2$ value is high, and the upper bound at 1 may be a cause for the generally skewed shape of the distribution. If we wanted to construct a confidence interval for $R^2$, we could take the 2.5% and 97.5% quantiles of the estimates across bootstrap samples. However, because the shape isn't symmetric, using the mean +/- 1.96 times the standard error probably wouldn't work well.

We can produce a distribution for $\log(\beta_0 * \beta1)$ using a similar approach, with a bit more wrangling before we make our plot.

```{r, cache=TRUE}
weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

As with $r^2$, this distribution is somewhat skewed and has some outliers. 

The point of this is not to say you should always use the bootstrap -- it's possible to establish "large sample" distributions for strange parameters / values / summaries in a lot of cases, and those are great to have. But it is helpful to know that there's a way to do inference even in tough cases. 



# Problem 2

Upload the `homicide-data` and create a `city_state` variable and a binary outcome `resolved_status` to indicate whether the homicide is solved. Omit a few `city_state` observations. Mutate a few character variables as factor variables or numeric variable for further analysis.
```{r}
homicide = read.csv("data/homicide-data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    city_state = str_c(city, ", ", state),
    resolved_status = case_when(
      disposition %in% c("Closed without arrest", "Open/No arrest") ~ "0",
      disposition == "Closed by arrest" ~ "1"
    ),
    resolved_status = as.factor(resolved_status),
    victim_age = as.numeric(victim_age),
    victim_race = as.factor(victim_race),
    victim_sex = as.factor(victim_sex)
  ) %>% 
  filter(
    !city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")
  ) %>% 
  drop_na(victim_age)
```


Filter the analysis for whom in `Baltimore, MD` and `victim_race` is `White` or `Black`.
Fit logistic regression using `glm`.
```{r}
homicide_analysis = homicide %>% 
  filter(
    victim_race %in% c("White", "Black"),
    city_state == "Baltimore, MD")

fit_logistic = homicide_analysis %>% 
  glm(resolved_status ~ victim_age + victim_sex + victim_race, family = binomial(), data = .)
  
fit_logistic = fit_logistic %>% 
  broom::tidy() %>% 
  mutate(
    OR = exp(estimate),
    LB = exp(estimate - 1.96*std.error),
    UB = exp(estimate + 1.96*std.error)
    ) %>% 
  filter(term == "victim_sexMale") %>% 
  select(term, estimate, OR, LB, UB) %>% 
  mutate_if(is.numeric, round, 3)


fit_logistic %>% 
  knitr::kable()
```

After limiting the analysis for whom `victim_race` is `white` or `black` and adjusting for victim age and victim race, the odds ratio for solving homicides comparing male victims to female victims is `r fit_logistic %>% select(OR)`. The 95% confidence interval for the adjusted odds ratio is (`r fit_logistic %>% select(LB)`, `r fit_logistic %>% select(UB)`).

Interpretation: The odds for solving homicides for male victims is`r fit_logistic %>% select(OR)` times the odds for solving homicides for female victims after adjusting for victim age and victim race. We are 95% confidence that the true odds ratio lies between `r fit_logistic %>% select(LB)` and `r fit_logistic %>% select(UB)`.


Write a `function(x)` for repeating the process of logisic regression.
```{r}

logit_reg = function(x){
  
  analysis = x %>% 
    filter(
    victim_race %in% c("White", "Black"))
  
  logit_reg = analysis %>% 
  glm(resolved_status ~ victim_age + victim_sex + victim_race, 
      family = binomial(), data = .) 
  
  logit_reg = logit_reg %>% 
  broom::tidy() %>% 
  mutate(
    OR = exp(estimate),
    LB = exp(estimate - 1.96*std.error),
    UB = exp(estimate + 1.96*std.error)
    ) %>% 
  filter(term == "victim_sexMale") %>% 
  select(term, estimate, OR, LB, UB) %>% 
  mutate_if(is.numeric, round, 3)
  
  logit_reg
}
```


Nesting the unrelated columns.
Map the nested data to the function `logit_reg` to iterate the process of logistic regression to each city.
```{r, cache = TRUE}
homicide_nest = homicide %>%
  select(city_state, everything()) %>% 
  nest(data = c(uid:disposition, resolved_status))


city_logistic = homicide_nest %>% 
  mutate(reg = map(data, logit_reg)) %>% 
  unnest(reg) %>% 
  select(city_state, OR, LB, UB)
 
city_logistic %>% 
  knitr::kable()
```

Create a plot that shows the estimate ORs and CIs for each city.

```{r}
city_logistic %>% 
  mutate(
    city_state = fct_reorder(city_state, OR)
  ) %>% 
  ggplot(aes(city_state, OR)) + 
  geom_point() +
  geom_errorbar(aes(ymin = LB, ymax = UB)) +
  labs(
    title = "The estimated ORs and CIs for each city",
    x = "City, State",
    y = "Estimated OR") +
  theme(axis.text.x = element_text(angle = 90))
```

New York, NY has the lowest odds ratio and Albuquerque, NM has the highest odds ratio for solving homicides comparing male victims to female victims. This finding suggests that male victims in New York, NY have the lowest odds for solving homicides comparing to female victims, whereas male victims in Albuquerque, NM have the highest odds for solving homicides comparing to female victims.

Also, Chicago, IL has the narrowest confidence interval for the odds ratio and Albuquerque, NM has the widest confidence interval for the odds ratio. The variation in the estimated ORs for Albuquerque, NM is the highest and for Chicago, IL is the lowest.



# Problem 3

Load the data and create the dataframe `birthweight`.
Mutate categorical variables `babysex`, `frace`, `malform`, `mrace` into factor variables and recode them using `recode_factor`.
Drop all the missing value using `drop_na`.
```{r}
birthweight = read.csv("data/birthweight.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    babysex = recode_factor(babysex, `1` = "male", `2` = "female"),
    frace = recode_factor(frace, `1` = "White", `2` = "Black", `3` = "Asian", `4` = "Puerto Rican", `8` = "Other", `9` = "Unknown"),
    malform = recode_factor(malform, `0` = "absent", `1` = "present"),
    mrace = recode_factor(mrace, `1` = "White", `2` = "Black", `3` = "Asian", `4` = "Puerto Rican", `8` = "Other")
  ) %>% 
  mutate_if(is.factor, fct_infreq) %>% 
  drop_na()
```


I would like to assess whether mother's age at delivery and mother's race would affect the baby's birthweight and whether the effect of mother's age on baby's birthweight would be different on different level of mother's race.
Fit a model with birthweight as the outcome and mother's age at delivery, mother's race, and their interaction as the predictors. 
```{r}
mymodel = lm(bwt ~ momage * mrace, data = birthweight)

birthweight %>% 
  add_residuals(mymodel) %>% 
  add_predictions(mymodel) %>%  
  ggplot(aes(x = pred, y = resid, color = mrace)) +
  geom_point() +
  labs(
    title = "Model residuals against fitted values",
    x = "Fitted values of birthweight",
    y = "Residuals")
  
```


Fit models: 
1)    using length at birth and gestational age as predictors
2)    using head circumference, length, sex and all interactions as predictors
```{r}
model1 = birthweight %>% 
  lm(bwt ~ blength + gaweeks, data = .) %>% 
  broom::tidy()

model2 = birthweight %>% 
  lm(bwt ~ bhead * blength * babysex, data = .) %>% 
  broom::tidy()
```


Compare my model with the two purposed models using `crossv_mc`.
Use `mutate`+`map` & `map2` to fit models to training data and obtain corresponding RMSEs for the testing data.
```{r}
cv_df =
  crossv_mc(birthweight, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df = 
  cv_df %>% 
  mutate(
    mymodel  = map(train, ~lm(bwt ~ momage * mrace, data = .x)),
    model1   = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    model2   = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x))) %>% 
  mutate(
    rmse_mymodel = map2_dbl(mymodel, test, ~rmse(model = .x, data = .y)),
    rmse_model1    = map2_dbl(model1, test, ~rmse(model = .x, data = .y)),
    rmse_model2 = map2_dbl(model2, test, ~rmse(model = .x, data = .y)))
```


Plot the prediction error distribution for each models. 
```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin() +
  labs(
    title = "The prediction error distribution for each candidate model",
    x = "Model",
    y = "RMSE") 
```

Since lower values of RMSE indicate better fit, the plot suggests that model 2, which uses head circumference, length, sex, and all interactions as the predictors, has the better fit.
My model which uses mother's age at delivery and mother's race as the predictors has the worst fit. 
