p8105_hw6_yx2711
================
Yingchen Xu
2022-11-30

# Problem 1

To obtain a distribution for $\hat{r}^2$, we’ll follow basically the
same procedure we used for regression coefficients: draw bootstrap
samples; the a model to each; extract the value I’m concerned with; and
summarize. Here, we’ll use `modelr::bootstrap` to draw the samples and
`broom::glance` to produce `r.squared` values.

``` r
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

``` r
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

<img src="p8105_hw6_yx2711_files/figure-gfm/unnamed-chunk-1-1.png" width="90%" />

In this example, the $\hat{r}^2$ value is high, and the upper bound at 1
may be a cause for the generally skewed shape of the distribution. If we
wanted to construct a confidence interval for $R^2$, we could take the
2.5% and 97.5% quantiles of the estimates across bootstrap samples.
However, because the shape isn’t symmetric, using the mean +/- 1.96
times the standard error probably wouldn’t work well.

We can produce a distribution for $\log(\beta_0 * \beta1)$ using a
similar approach, with a bit more wrangling before we make our plot.

``` r
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

<img src="p8105_hw6_yx2711_files/figure-gfm/unnamed-chunk-2-1.png" width="90%" />

As with $r^2$, this distribution is somewhat skewed and has some
outliers.

The point of this is not to say you should always use the bootstrap –
it’s possible to establish “large sample” distributions for strange
parameters / values / summaries in a lot of cases, and those are great
to have. But it is helpful to know that there’s a way to do inference
even in tough cases.

# Problem 2

Upload the `homicide-data` and create a `city_state` variable and a
binary outcome `resolved_status` to indicate whether the homicide is
solved. Omit a few `city_state` observations. Mutate a few character
variables as factor variables or numeric variable for further analysis.

``` r
homicide = read.csv("data/homicide-data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    city_state = str_c(city, ", ", state),
    resolved_status = case_when(
      disposition %in% c("Closed without arrest", "Open/No arrest") ~ "0",
      disposition == "Closed by arrest" ~ "1"
    ),
    resolved_status = as.factor(resolved_status),
    victim_age = as.numeric(victim_age),
    victim_race = as.factor(victim_race),
    victim_sex = as.factor(victim_sex)
  ) %>% 
  filter(
    !city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")
  ) %>% 
  drop_na(victim_age)
```

Filter the analysis for whom in `Baltimore, MD` and `victim_race` is
`White` or `Black`. Fit logistic regression using `glm`.

``` r
homicide_analysis = homicide %>% 
  filter(
    victim_race %in% c("White", "Black"),
    city_state == "Baltimore, MD")

fit_logistic = homicide_analysis %>% 
  glm(resolved_status ~ victim_age + victim_sex + victim_race, family = binomial(), data = .)
  
fit_logistic = fit_logistic %>% 
  broom::tidy() %>% 
  mutate(
    OR = exp(estimate),
    LB = exp(estimate - 1.96*std.error),
    UB = exp(estimate + 1.96*std.error)
    ) %>% 
  filter(term == "victim_sexMale") %>% 
  select(term, estimate, OR, LB, UB) %>% 
  mutate_if(is.numeric, round, 3)


fit_logistic %>% 
  knitr::kable()
```

| term           | estimate |    OR |    LB |    UB |
|:---------------|---------:|------:|------:|------:|
| victim_sexMale |   -0.854 | 0.426 | 0.325 | 0.558 |

After adjusting for victim age and victim race, the odds ratio for
solving homicides comparing male victims to female victims is 0.426. The
95% confidence interval for the adjusted odds ratio is (0.325, 0.558).

Interpretation: The odds for solving homicides for male victims is0.426
times the odds for solving homicides for female victims. We are 95%
confidence that the true odds ratio lies between 0.325 and 0.558.

Write a `function(x)` for repeating the process of logisic regression.

``` r
logit_reg = function(x){
  
  analysis = x %>% 
    filter(
    victim_race %in% c("White", "Black"))
  
  logit_reg = analysis %>% 
  glm(resolved_status ~ victim_age + victim_sex + victim_race, 
      family = binomial(), data = .) 
  
  logit_reg = logit_reg %>% 
  broom::tidy() %>% 
  mutate(
    OR = exp(estimate),
    LB = exp(estimate - 1.96*std.error),
    UB = exp(estimate + 1.96*std.error)
    ) %>% 
  filter(term == "victim_sexMale") %>% 
  select(term, estimate, OR, LB, UB) %>% 
  mutate_if(is.numeric, round, 3)
  
  logit_reg
}
```

Nesting the unrelated columns. Map the nested data to the function
`logit_reg` to iterate the process of logistic regression to each city.

``` r
homicide_nest = homicide %>%
  select(city_state, everything()) %>% 
  nest(data = c(uid:disposition, resolved_status))


city_logistic = homicide_nest %>% 
  mutate(reg = map(data, logit_reg)) %>% 
  unnest(reg) %>% 
  select(city_state, OR, LB, UB)
 
city_logistic %>% 
  knitr::kable()
```

| city_state         |    OR |    LB |    UB |
|:-------------------|------:|------:|------:|
| Albuquerque, NM    | 1.767 | 0.831 | 3.761 |
| Atlanta, GA        | 1.000 | 0.684 | 1.463 |
| Baltimore, MD      | 0.426 | 0.325 | 0.558 |
| Baton Rouge, LA    | 0.381 | 0.209 | 0.695 |
| Birmingham, AL     | 0.870 | 0.574 | 1.318 |
| Boston, MA         | 0.674 | 0.356 | 1.276 |
| Buffalo, NY        | 0.521 | 0.290 | 0.935 |
| Charlotte, NC      | 0.884 | 0.557 | 1.403 |
| Chicago, IL        | 0.410 | 0.336 | 0.501 |
| Cincinnati, OH     | 0.400 | 0.236 | 0.677 |
| Columbus, OH       | 0.532 | 0.378 | 0.750 |
| Denver, CO         | 0.479 | 0.236 | 0.971 |
| Detroit, MI        | 0.582 | 0.462 | 0.734 |
| Durham, NC         | 0.812 | 0.392 | 1.683 |
| Fort Worth, TX     | 0.669 | 0.397 | 1.127 |
| Fresno, CA         | 1.335 | 0.580 | 3.071 |
| Houston, TX        | 0.711 | 0.558 | 0.907 |
| Indianapolis, IN   | 0.919 | 0.679 | 1.242 |
| Jacksonville, FL   | 0.720 | 0.537 | 0.966 |
| Las Vegas, NV      | 0.837 | 0.608 | 1.154 |
| Long Beach, CA     | 0.410 | 0.156 | 1.082 |
| Los Angeles, CA    | 0.662 | 0.458 | 0.956 |
| Louisville, KY     | 0.491 | 0.305 | 0.790 |
| Memphis, TN        | 0.723 | 0.529 | 0.988 |
| Miami, FL          | 0.515 | 0.304 | 0.872 |
| Milwaukee, wI      | 0.727 | 0.499 | 1.060 |
| Minneapolis, MN    | 0.947 | 0.478 | 1.875 |
| Nashville, TN      | 1.034 | 0.685 | 1.562 |
| New Orleans, LA    | 0.585 | 0.422 | 0.811 |
| New York, NY       | 0.262 | 0.138 | 0.499 |
| Oakland, CA        | 0.563 | 0.365 | 0.868 |
| Oklahoma City, OK  | 0.974 | 0.624 | 1.520 |
| Omaha, NE          | 0.382 | 0.203 | 0.721 |
| Philadelphia, PA   | 0.496 | 0.378 | 0.652 |
| Pittsburgh, PA     | 0.431 | 0.265 | 0.700 |
| Richmond, VA       | 1.006 | 0.498 | 2.033 |
| San Antonio, TX    | 0.705 | 0.398 | 1.249 |
| Sacramento, CA     | 0.669 | 0.335 | 1.337 |
| Savannah, GA       | 0.867 | 0.422 | 1.780 |
| San Bernardino, CA | 0.500 | 0.171 | 1.462 |
| San Diego, CA      | 0.413 | 0.200 | 0.855 |
| San Francisco, CA  | 0.608 | 0.317 | 1.165 |
| St. Louis, MO      | 0.703 | 0.530 | 0.932 |
| Stockton, CA       | 1.352 | 0.621 | 2.942 |
| Tampa, FL          | 0.808 | 0.348 | 1.876 |
| Tulsa, OK          | 0.976 | 0.614 | 1.552 |
| Washington, DC     | 0.690 | 0.468 | 1.017 |

# Problem 3

``` r
birthweight = read.csv("data/birthweight.csv") %>% 
  janitor::clean_names()
```
